{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional machine learning\n",
    "Feature(s): 'SMILES' column\n",
    "\n",
    "- 'SMILES' column\n",
    "\n",
    "Target: E_bin\n",
    "\n",
    "- Binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import clone\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"WARNING\")\n",
    "# Append the parent directory of your package to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique smiles: 966\n",
      "Count of all of the smiles: 966\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "path_to_dataset = 'train_denis.csv'\n",
    "csv_filename = 'train_denis.csv'\n",
    "\n",
    "# Open the file, Correct the encoding and sep if necessary\n",
    "if path_to_dataset.endswith('.zip'):\n",
    "    with zipfile.ZipFile(path_to_dataset, 'r') as z:\n",
    "        # Open the CSV file within the ZIP file\n",
    "        with z.open(csv_filename) as f:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(f, sep=',', on_bad_lines='warn', index_col = 0)\n",
    "else:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(path_to_dataset, sep=',', on_bad_lines='warn', index_col = 0)\n",
    "\n",
    "\n",
    "print('Count of unique smiles:', df.SMILES.unique().shape[0])\n",
    "print('Count of all of the smiles:', df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsmo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from MLPipeline import MLmodel, BinTheTarget\n",
    "\n",
    "Target = ['E_bin']\n",
    "Features = ['SMILES']\n",
    "Feature_types = ['SMILES']\n",
    "input = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveRandomForestClassifier(trial, model_instance):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize.\n",
    "    \"\"\"\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [None, 10, 20, 30, 40]),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 6),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "    # Clone the model to ensure a fresh instance each trial\n",
    "    model_clone = clone(model_instance.model)\n",
    "    model_clone.set_params(**params)\n",
    "    \n",
    "    # Define the score metric\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model_clone, model_instance.X_train, model_instance.y_train, cv=model_instance.cv, scoring=scoring)\n",
    "\n",
    "    # Return the average score across all folds\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def objectiveXGBClassifier(trial, model_instance):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize for XGBClassifier.\n",
    "    \"\"\"\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),  # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),  # L2 regularization\n",
    "    }\n",
    "\n",
    "    # Clone the model to ensure a fresh instance each trial\n",
    "    model_clone = clone(model_instance.model)\n",
    "    model_clone.set_params(**params)\n",
    "    \n",
    "    # Define the score metric\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model_clone, model_instance.X_train, model_instance.y_train, cv=model_instance.cv, scoring=scoring)\n",
    "\n",
    "    # Return the average score across all folds\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 10:59:49,095] A new study created in memory with name: no-name-28d8d670-1606-4dc5-bde0-a84c4a7acacb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:RandomForestClassifier / Target:['E_bin'] / Train size:25 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 10:59:50,197] Trial 0 finished with value: 0.72 and parameters: {'n_estimators': 250, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-09-17 10:59:51,615] Trial 1 finished with value: 0.64 and parameters: {'n_estimators': 211, 'max_depth': 20, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-09-17 10:59:51,982] Trial 2 finished with value: 0.64 and parameters: {'n_estimators': 63, 'max_depth': None, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-09-17 10:59:53,021] Trial 3 finished with value: 0.64 and parameters: {'n_estimators': 166, 'max_depth': 30, 'min_samples_split': 14, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-09-17 10:59:53,935] Trial 4 finished with value: 0.76 and parameters: {'n_estimators': 212, 'max_depth': 10, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:55,824] Trial 5 finished with value: 0.6799999999999999 and parameters: {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:57,170] Trial 6 finished with value: 0.64 and parameters: {'n_estimators': 229, 'max_depth': 20, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:57,846] Trial 7 finished with value: 0.68 and parameters: {'n_estimators': 148, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:58,258] Trial 8 finished with value: 0.64 and parameters: {'n_estimators': 65, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:58,797] Trial 9 finished with value: 0.72 and parameters: {'n_estimators': 122, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 4 with value: 0.76.\n",
      "[I 2024-09-17 10:59:59,119] A new study created in memory with name: no-name-63a78d8b-c16e-4011-bbb3-f43b55e8c07a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:RandomForestClassifier / Target:['E_bin'] / Train size:50 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 11:00:00,377] Trial 0 finished with value: 0.7000000000000001 and parameters: {'n_estimators': 237, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 0 with value: 0.7000000000000001.\n",
      "[I 2024-09-17 11:00:01,894] Trial 1 finished with value: 0.74 and parameters: {'n_estimators': 245, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:02,863] Trial 2 finished with value: 0.72 and parameters: {'n_estimators': 220, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:03,635] Trial 3 finished with value: 0.64 and parameters: {'n_estimators': 179, 'max_depth': None, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:04,904] Trial 4 finished with value: 0.7000000000000001 and parameters: {'n_estimators': 300, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:05,296] Trial 5 finished with value: 0.74 and parameters: {'n_estimators': 85, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:06,119] Trial 6 finished with value: 0.72 and parameters: {'n_estimators': 134, 'max_depth': None, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:06,870] Trial 7 finished with value: 0.72 and parameters: {'n_estimators': 166, 'max_depth': None, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False}. Best is trial 1 with value: 0.74.\n",
      "[I 2024-09-17 11:00:07,307] Trial 8 finished with value: 0.76 and parameters: {'n_estimators': 105, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False}. Best is trial 8 with value: 0.76.\n",
      "[I 2024-09-17 11:00:07,949] Trial 9 finished with value: 0.7 and parameters: {'n_estimators': 111, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True}. Best is trial 8 with value: 0.76.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:RandomForestClassifier / Target:['E_bin'] / Train size:100 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 11:00:08,252] A new study created in memory with name: no-name-b3c041e3-b489-4d0d-9ed1-67bf55ed2560\n",
      "[I 2024-09-17 11:00:09,903] Trial 0 finished with value: 0.67 and parameters: {'n_estimators': 273, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 0.67.\n",
      "[I 2024-09-17 11:00:11,685] Trial 1 finished with value: 0.7 and parameters: {'n_estimators': 246, 'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}. Best is trial 1 with value: 0.7.\n",
      "[I 2024-09-17 11:00:11,949] Trial 2 finished with value: 0.7699999999999999 and parameters: {'n_estimators': 59, 'max_depth': 40, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:13,004] Trial 3 finished with value: 0.72 and parameters: {'n_estimators': 221, 'max_depth': 40, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:13,589] Trial 4 finished with value: 0.73 and parameters: {'n_estimators': 94, 'max_depth': None, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:14,455] Trial 5 finished with value: 0.72 and parameters: {'n_estimators': 136, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:15,845] Trial 6 finished with value: 0.73 and parameters: {'n_estimators': 223, 'max_depth': 20, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:16,783] Trial 7 finished with value: 0.76 and parameters: {'n_estimators': 151, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:17,223] Trial 8 finished with value: 0.75 and parameters: {'n_estimators': 72, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.7699999999999999.\n",
      "[I 2024-09-17 11:00:17,477] Trial 9 finished with value: 0.71 and parameters: {'n_estimators': 56, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False}. Best is trial 2 with value: 0.7699999999999999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:RandomForestClassifier / Target:['E_bin'] / Train size:200 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 11:00:17,908] A new study created in memory with name: no-name-680dedce-b664-4384-add6-3f02d906b65e\n",
      "[I 2024-09-17 11:00:18,647] Trial 0 finished with value: 0.8150000000000001 and parameters: {'n_estimators': 152, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False}. Best is trial 0 with value: 0.8150000000000001.\n",
      "[I 2024-09-17 11:00:20,486] Trial 1 finished with value: 0.7699999999999999 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 0.8150000000000001.\n",
      "[I 2024-09-17 11:00:21,758] Trial 2 finished with value: 0.835 and parameters: {'n_estimators': 193, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:23,479] Trial 3 finished with value: 0.8099999999999999 and parameters: {'n_estimators': 205, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:24,896] Trial 4 finished with value: 0.835 and parameters: {'n_estimators': 276, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:26,855] Trial 5 finished with value: 0.8150000000000001 and parameters: {'n_estimators': 276, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:27,548] Trial 6 finished with value: 0.7950000000000002 and parameters: {'n_estimators': 81, 'max_depth': None, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:29,024] Trial 7 finished with value: 0.79 and parameters: {'n_estimators': 239, 'max_depth': None, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:30,611] Trial 8 finished with value: 0.835 and parameters: {'n_estimators': 293, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:32,305] Trial 9 finished with value: 0.8100000000000002 and parameters: {'n_estimators': 245, 'max_depth': 30, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.835.\n",
      "[I 2024-09-17 11:00:32,662] A new study created in memory with name: no-name-f6153b69-a7e5-499c-82b3-237c8a4987b2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:XGBClassifier / Target:['E_bin'] / Train size:25 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 11:01:02,670] Trial 0 finished with value: 0.8 and parameters: {'n_estimators': 271, 'max_depth': 9, 'learning_rate': 0.10118370066597199, 'subsample': 0.60546187040602, 'colsample_bytree': 0.5840350017676692, 'gamma': 0.6954148586452794, 'reg_alpha': 0.0003920353582371209, 'reg_lambda': 0.0003123958149347385}. Best is trial 0 with value: 0.8.\n",
      "[I 2024-09-17 11:01:22,343] Trial 1 finished with value: 0.8400000000000001 and parameters: {'n_estimators': 161, 'max_depth': 13, 'learning_rate': 0.11630523602185806, 'subsample': 0.5960048577648127, 'colsample_bytree': 0.7482304835642122, 'gamma': 0.0002974028882593881, 'reg_alpha': 0.4909918718936664, 'reg_lambda': 2.2815255803913534e-05}. Best is trial 1 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:01:49,472] Trial 2 finished with value: 0.76 and parameters: {'n_estimators': 153, 'max_depth': 4, 'learning_rate': 0.06789230172304099, 'subsample': 0.7167660915316689, 'colsample_bytree': 0.8002850860508115, 'gamma': 0.0005070486962071262, 'reg_alpha': 6.451495343711184e-08, 'reg_lambda': 4.276724671529787e-08}. Best is trial 1 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:02:21,843] Trial 3 finished with value: 0.76 and parameters: {'n_estimators': 273, 'max_depth': 13, 'learning_rate': 0.08049154079938974, 'subsample': 0.5172986747465591, 'colsample_bytree': 0.6499536942119877, 'gamma': 8.179824258374129e-07, 'reg_alpha': 0.010724439582409382, 'reg_lambda': 5.535965912252535e-07}. Best is trial 1 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:03:06,735] Trial 4 finished with value: 0.8 and parameters: {'n_estimators': 279, 'max_depth': 10, 'learning_rate': 0.01905175130936031, 'subsample': 0.6444480664691374, 'colsample_bytree': 0.6965161007922569, 'gamma': 4.814148856605205e-06, 'reg_alpha': 0.029947663752436477, 'reg_lambda': 4.04297645411019e-05}. Best is trial 1 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:03:46,603] Trial 5 finished with value: 0.76 and parameters: {'n_estimators': 271, 'max_depth': 8, 'learning_rate': 0.13407449646219965, 'subsample': 0.6171535949901775, 'colsample_bytree': 0.9179368709746618, 'gamma': 0.006911071144338619, 'reg_alpha': 6.690113852266998e-07, 'reg_lambda': 7.225366523668816e-06}. Best is trial 1 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:04:20,249] Trial 6 finished with value: 0.8800000000000001 and parameters: {'n_estimators': 244, 'max_depth': 12, 'learning_rate': 0.11144048701613785, 'subsample': 0.9740604043960382, 'colsample_bytree': 0.8883941174092163, 'gamma': 0.2667554283080119, 'reg_alpha': 0.00039798724282806347, 'reg_lambda': 0.013822455163062405}. Best is trial 6 with value: 0.8800000000000001.\n",
      "[I 2024-09-17 11:04:35,513] Trial 7 finished with value: 0.8 and parameters: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.23173452996863694, 'subsample': 0.8039017019724315, 'colsample_bytree': 0.7505238970469797, 'gamma': 0.001987760462894121, 'reg_alpha': 0.00012822945648320846, 'reg_lambda': 3.9501007552761476e-05}. Best is trial 6 with value: 0.8800000000000001.\n",
      "[I 2024-09-17 11:04:57,724] Trial 8 finished with value: 0.76 and parameters: {'n_estimators': 266, 'max_depth': 6, 'learning_rate': 0.09906875570423104, 'subsample': 0.9295834913755814, 'colsample_bytree': 0.8119607210304819, 'gamma': 7.916658518357431e-05, 'reg_alpha': 0.0007485756678293747, 'reg_lambda': 0.0020377762219565975}. Best is trial 6 with value: 0.8800000000000001.\n",
      "[I 2024-09-17 11:05:12,084] Trial 9 finished with value: 0.9199999999999999 and parameters: {'n_estimators': 60, 'max_depth': 4, 'learning_rate': 0.03924418852583167, 'subsample': 0.949979005154747, 'colsample_bytree': 0.509423039553084, 'gamma': 0.9177764407235685, 'reg_alpha': 3.274935015675225e-05, 'reg_lambda': 1.6631596364029727e-05}. Best is trial 9 with value: 0.9199999999999999.\n",
      "[I 2024-09-17 11:05:13,766] A new study created in memory with name: no-name-456aa954-1c7a-457e-a6ba-e8ba1bd987f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: Model:XGBClassifier / Target:['E_bin'] / Train size:50 / Seed:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 11:05:30,890] Trial 0 finished with value: 0.8400000000000001 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05078224679429391, 'subsample': 0.6232213242057594, 'colsample_bytree': 0.9645395995016721, 'gamma': 0.04479475534115711, 'reg_alpha': 1.0485707591112221e-06, 'reg_lambda': 0.005516185205685083}. Best is trial 0 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:05:37,764] Trial 1 finished with value: 0.8400000000000001 and parameters: {'n_estimators': 150, 'max_depth': 14, 'learning_rate': 0.064416308784722, 'subsample': 0.9810493825737376, 'colsample_bytree': 0.9938474138177816, 'gamma': 0.024471379041587235, 'reg_alpha': 6.887768530693431e-05, 'reg_lambda': 5.3484172043224405e-06}. Best is trial 0 with value: 0.8400000000000001.\n",
      "[I 2024-09-17 11:06:06,396] Trial 2 finished with value: 0.8400000000000001 and parameters: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.021206790586638886, 'subsample': 0.6498342856323154, 'colsample_bytree': 0.9213899169847832, 'gamma': 0.04065114161272321, 'reg_alpha': 0.00044582146615261656, 'reg_lambda': 5.560953927811918e-06}. Best is trial 0 with value: 0.8400000000000001.\n",
      "[W 2024-09-17 11:06:32,025] Trial 3 failed with parameters: {'n_estimators': 224, 'max_depth': 9, 'learning_rate': 0.020410525826091142, 'subsample': 0.6469881101687651, 'colsample_bytree': 0.9906308827089503, 'gamma': 2.3348095534306504e-07, 'reg_alpha': 0.04981015376008219, 'reg_lambda': 0.00024998113437668386} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3622809/4136497206.py\", line 28, in <lambda>\n",
      "    objective=lambda trial: objective(trial, model)\n",
      "  File \"/tmp/ipykernel_3622809/3340104105.py\", line 53, in objectiveXGBClassifier\n",
      "    scores = cross_val_score(model_clone, model_instance.X_train, model_instance.y_train, cv=model_instance.cv, scoring=scoring)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1515, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/home/lsmo/.local/lib/python3.10/site-packages/xgboost/core.py\", line 2050, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-09-17 11:06:32,029] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     15\u001b[0m     objective \u001b[38;5;241m=\u001b[39m objectiveXGBClassifier\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m MLmodel(modelType\u001b[38;5;241m=\u001b[39mmodel_name, \n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     19\u001b[0m randomSeed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, model)\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28meval\u001b[39m, summary \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     33\u001b[0m result\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msummary, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28meval\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m:seed})\n",
      "File \u001b[0;32m~/Desktop/Joren_ML/GPT-Challenge/MLPipeline.py:236\u001b[0m, in \u001b[0;36mMLmodel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimization_method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid optimization method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimization_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoose from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Train the model without hyperparameter tuning\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n",
      "File \u001b[0;32m~/Desktop/Joren_ML/GPT-Challenge/MLPipeline.py:206\u001b[0m, in \u001b[0;36mMLmodel.hyperparameter_optimization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimization_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    205\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodelType \u001b[38;5;129;01min\u001b[39;00m Classification_models \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimization_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/JorenML/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     15\u001b[0m     objective \u001b[38;5;241m=\u001b[39m objectiveXGBClassifier\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m MLmodel(modelType\u001b[38;5;241m=\u001b[39mmodel_name, \n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     19\u001b[0m randomSeed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m     20\u001b[0m train_count \u001b[38;5;241m=\u001b[39m train_size,\n\u001b[1;32m     21\u001b[0m test_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m     22\u001b[0m target\u001b[38;5;241m=\u001b[39mtarget, \n\u001b[1;32m     23\u001b[0m features\u001b[38;5;241m=\u001b[39mFeatures, \n\u001b[1;32m     24\u001b[0m hyperparameter_tuning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m feature_types\u001b[38;5;241m=\u001b[39mFeature_types,\n\u001b[1;32m     26\u001b[0m optimization_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     27\u001b[0m optimization_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m---> 28\u001b[0m objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28meval\u001b[39m, summary \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mobjectiveXGBClassifier\u001b[0;34m(trial, model_instance)\u001b[0m\n\u001b[1;32m     50\u001b[0m scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_clone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Return the average score across all folds\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:1515\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1487\u001b[0m (\n\u001b[1;32m   1488\u001b[0m     model,\n\u001b[1;32m   1489\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1495\u001b[0m )\n\u001b[1;32m   1496\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1497\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1498\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1513\u001b[0m )\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     _check_call(\n\u001b[0;32m-> 2050\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NAMES = ['RandomForestClassifier', 'XGBClassifier']\n",
    "TARGETS = [[i] for i in Target ]\n",
    "TRAIN_SIZES = [25, 50, 100, 200]\n",
    "\n",
    "result = []\n",
    "for seed in [1, 2, 3]:\n",
    "    for model_name in MODEL_NAMES:\n",
    "        for target in TARGETS:\n",
    "            for train_size in TRAIN_SIZES:\n",
    "                print(f'RUN: Model:{model_name} / Target:{target} / Train size:{train_size} / Seed:{seed}')\n",
    "                \n",
    "                if model_name == 'RandomForestClassifier':\n",
    "                    objective = objectiveRandomForestClassifier\n",
    "                elif model_name == 'XGBClassifier':\n",
    "                    objective = objectiveXGBClassifier\n",
    "\n",
    "                model = MLmodel(modelType=model_name, \n",
    "                df=input,\n",
    "                randomSeed=seed,\n",
    "                train_count = train_size,\n",
    "                test_count = 50, \n",
    "                target=target, \n",
    "                features=Features, \n",
    "                hyperparameter_tuning=True,\n",
    "                feature_types=Feature_types,\n",
    "                optimization_method='optuna', \n",
    "                optimization_trials=10,\n",
    "                objective=lambda trial: objective(trial, model)\n",
    "                )\n",
    "\n",
    "                model.train()\n",
    "                eval, summary = model.evaluate()\n",
    "                result.append({**summary, **eval, 'seed':seed})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "display(df)\n",
    "df.to_csv('ActivationEnergy_cycloadditions_traditional_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "METRICS = ['accuracy','f1_micro','f1_macro', 'kappa']\n",
    "fig, ax = plt.subplots(len(METRICS), 1, sharex=True, layout = 'constrained')\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, metric in enumerate(METRICS):\n",
    "    ax[i].plot(df['train_size'], df[metric], marker = 'o')\n",
    "    ax[i].set_title(metric)\n",
    "    ax[i].set_ylim(0,1)\n",
    "\n",
    "fig.suptitle('Random Forrest & XGB - ActivationEnergy_cycloadditions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
